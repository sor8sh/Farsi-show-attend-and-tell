{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIC.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FWEPgulF-6Wi",
        "mKBFA3SaaqY9",
        "296u64vRvvC_",
        "Bsf8jwNh4BkP",
        "e9pWq2Eg4kGG",
        "NEYJvdSmDRnG",
        "DsLY9FFQDkG1",
        "rQo128TYDqrc",
        "NyB9RMpVD2S0",
        "-Nozw9_4D8YK",
        "oSMEM6HNEjnA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWEPgulF-6Wi"
      },
      "source": [
        "# Don't run this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iUDmZ21-_O-"
      },
      "source": [
        "%cd\n",
        "%cd ../content\n",
        "!rm -r summary\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLy4krqH_tnv"
      },
      "source": [
        "%cd\n",
        "%cd ../content/utils\n",
        "!mkdir utils\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkjQELZfE2St"
      },
      "source": [
        "%cd\n",
        "%cd ../content\n",
        "!mv models \"gdrive/My Drive/Final Project/TensorFlowImplementation/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s85AUPq9lZDu"
      },
      "source": [
        "%cd\n",
        "%cd ../content\n",
        "!wget -cq https://public.boxcloud.com/d/1/b1!3ZUrqhCq8itO2YnJ4w9ILQzBsd3LX5OUY3wlbiEQqt81zeELbRgi0XHkF9MZYhyrWTNUDhe00YGjpR1Xdrdlu5SskgIowpbex2syMXsMLqxK9rOMthKV2cDQpBbim2UcDX-5n34AJHN3jeoBq3nDgogZvLS92qFtV3-JV4NGP0M65NPyNVfTze_XCUx9o4GlumFxowKp_7PiKG0miR2efDyODzRwhKeS3TVs2w_pv6g_rp4HqI3qEyw-tz0JHwxEHZpU5Ua11hreTlDC9pO2tftRTS6JE4w5P77Y5RgKQbaDFYHnB7rFQfzFBSsSi7Id15rquhLXwOQbXlxomPY67zISAD1Z2J_59Lm52x4guvCewpNay5ImehJ6lbShmKB2N-jLGWzf79E6knsklPv4PcLsO5svvVY5NIvlR7ZRsN9AJJ2IoeYHFBPmonEdwTHeXQlNyDs2yiW5ilNkkyYfNCIaBvUCK-p74eN1qNK9v19X31n8a5ErdqX_Ls3HuUYzDZX_49VNzxY1cokazD77wWN_rLU62UUUk1DmLbFAizc7Llbsxc4MKcpchy8_vnWCSQd2TBjscyAXYje4K09963qezBWCvOmeKuEwPaeONF1rBXnWvKZ22dgkgudw2FGXJln2d-JzprECMDmgAJR3h9H25yMLYvm2ddYdGsPHhlvVRg4x6BejjCmt_i60AZeMy2M3TVFPLKbMHEjy7z_n3bT7-Tsx2fDfOYWkwzNwktNT18vyvr9cNWnoCQYR6B7ujVaaPzslmeo0o9Mh5WbycwMIaa9H9m_QZJDHTY_WK1lUh6pg-rI45o17IMdoDboIDKqa_dYpCDrvbBoCh39A-g1f8V5BU_NDp5CGpmkLrb2j13c0N9-9Jz4sc5ZkyRs8xmCV9UAiF3LDFI9SyZIrDWKP4x8wZBA_1jVleCM7VfGtsfYMXqQgzecdn5Ph7Dm3nrhgvO0EJTlPGqkK8bVem_36ImX6EXMHdjEm8ocM0gw4QKPU-mfGgbdQfX50IWYQIBFiLvUQorjhalnERS27m9LypNJcKkfLz_uzqb6XUtIAtLO4khAqDnLAbt19Hmi5yopT8sYyQk7fmZHkjnMDp61HrJ8fF2mcWPCcUyJkIY1QA7u9q6uM187Ne9PUxHz-27HndU8KY9aBcFpKxfJv50BUVZdfX1kI4SWTqhz5tDfge2pfSzjHmGKaE5N0FTZMmn2fBw1fK8Akbai3yhfgv1TmYlhreS6XZipPc5PgdoWAI--3/download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkAJ_AqnSto9"
      },
      "source": [
        "%cd\n",
        "# %cd ../content\n",
        "!cp -r '../content/gdrive/My Drive/Final Project/TensorFlowImplementation/utils' ../content/\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiW5-N_K3Bn"
      },
      "source": [
        "%cd \n",
        "%cd ../content\n",
        "!rm -r test/results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOE2erCeLX6f"
      },
      "source": [
        "%cd\n",
        "%cd \n",
        "# !cp -r ../content/test/images ../content\n",
        "!cp -r ../content/images ../content/test/\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knBe1SG4PBXn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "700a74af-32bf-47c2-cb22-e72a64ef0967"
      },
      "source": [
        "%cd\n",
        "%cd ../content/test\n",
        "!tar -zcvf archive2.tar.gz results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/test\n",
            "results/\n",
            "results/COCO_test2014_000000127804_result.jpg\n",
            "results/COCO_test2014_000000030652_result.jpg\n",
            "results/COCO_test2014_000000392553_result.jpg\n",
            "results/COCO_test2014_000000403022_result.jpg\n",
            "results/COCO_test2014_000000502521_result.jpg\n",
            "results/COCO_test2014_000000145224_result.jpg\n",
            "results/COCO_test2014_000000201536_result.jpg\n",
            "results/COCO_test2014_000000524509_result.jpg\n",
            "results/COCO_test2014_000000198343_result.jpg\n",
            "results/COCO_test2014_000000377771_result.jpg\n",
            "results/COCO_test2014_000000170557_result.jpg\n",
            "results/COCO_test2014_000000235690_result.jpg\n",
            "results/COCO_test2014_000000282739_result.jpg\n",
            "results/COCO_test2014_000000074368_result.jpg\n",
            "results/COCO_test2014_000000371386_result.jpg\n",
            "results/COCO_test2014_000000549751_result.jpg\n",
            "results/COCO_test2014_000000216821_result.jpg\n",
            "results/COCO_test2014_000000016803_result.jpg\n",
            "results/COCO_test2014_000000147845_result.jpg\n",
            "results/COCO_test2014_000000509912_result.jpg\n",
            "results/COCO_test2014_000000129034_result.jpg\n",
            "results/COCO_test2014_000000075328_result.jpg\n",
            "results/COCO_test2014_000000479215_result.jpg\n",
            "results/COCO_test2014_000000396125_result.jpg\n",
            "results/COCO_test2014_000000318920_result.jpg\n",
            "results/COCO_test2014_000000485104_result.jpg\n",
            "results/COCO_test2014_000000478366_result.jpg\n",
            "results/COCO_test2014_000000402535_result.jpg\n",
            "results/COCO_test2014_000000467877_result.jpg\n",
            "results/COCO_test2014_000000247972_result.jpg\n",
            "results/COCO_test2014_000000064382_result.jpg\n",
            "results/COCO_test2014_000000094626_result.jpg\n",
            "results/COCO_test2014_000000550668_result.jpg\n",
            "results/COCO_test2014_000000375154_result.jpg\n",
            "results/COCO_test2014_000000496295_result.jpg\n",
            "results/COCO_test2014_000000089014_result.jpg\n",
            "results/COCO_test2014_000000501895_result.jpg\n",
            "results/COCO_test2014_000000377066_result.jpg\n",
            "results/COCO_test2014_000000334624_result.jpg\n",
            "results/COCO_test2014_000000201331_result.jpg\n",
            "results/COCO_test2014_000000543001_result.jpg\n",
            "results/COCO_test2014_000000380805_result.jpg\n",
            "results/COCO_test2014_000000007493_result.jpg\n",
            "results/COCO_test2014_000000043205_result.jpg\n",
            "results/COCO_test2014_000000329309_result.jpg\n",
            "results/COCO_test2014_000000506240_result.jpg\n",
            "results/COCO_test2014_000000083946_result.jpg\n",
            "results/COCO_test2014_000000172443_result.jpg\n",
            "results/COCO_test2014_000000300536_result.jpg\n",
            "results/COCO_test2014_000000274787_result.jpg\n",
            "results/COCO_test2014_000000083486_result.jpg\n",
            "results/COCO_test2014_000000064051_result.jpg\n",
            "results/COCO_test2014_000000304198_result.jpg\n",
            "results/COCO_test2014_000000440976_result.jpg\n",
            "results/COCO_test2014_000000305486_result.jpg\n",
            "results/COCO_test2014_000000355079_result.jpg\n",
            "results/COCO_test2014_000000207767_result.jpg\n",
            "results/COCO_test2014_000000222660_result.jpg\n",
            "results/COCO_test2014_000000217597_result.jpg\n",
            "results/COCO_test2014_000000316239_result.jpg\n",
            "results/COCO_test2014_000000369760_result.jpg\n",
            "results/COCO_test2014_000000458001_result.jpg\n",
            "results/COCO_test2014_000000107463_result.jpg\n",
            "results/COCO_test2014_000000124170_result.jpg\n",
            "results/COCO_test2014_000000092100_result.jpg\n",
            "results/COCO_test2014_000000374937_result.jpg\n",
            "results/COCO_test2014_000000070507_result.jpg\n",
            "results/COCO_test2014_000000349113_result.jpg\n",
            "results/COCO_test2014_000000512865_result.jpg\n",
            "results/COCO_test2014_000000494350_result.jpg\n",
            "results/COCO_test2014_000000324998_result.jpg\n",
            "results/COCO_test2014_000000260824_result.jpg\n",
            "results/COCO_test2014_000000304464_result.jpg\n",
            "results/COCO_test2014_000000492442_result.jpg\n",
            "results/COCO_test2014_000000009191_result.jpg\n",
            "results/COCO_test2014_000000422837_result.jpg\n",
            "results/COCO_test2014_000000159368_result.jpg\n",
            "results/COCO_test2014_000000495924_result.jpg\n",
            "results/COCO_test2014_000000115553_result.jpg\n",
            "results/COCO_test2014_000000536816_result.jpg\n",
            "results/COCO_test2014_000000102419_result.jpg\n",
            "results/COCO_test2014_000000468397_result.jpg\n",
            "results/COCO_test2014_000000063459_result.jpg\n",
            "results/COCO_test2014_000000451187_result.jpg\n",
            "results/COCO_test2014_000000114154_result.jpg\n",
            "results/COCO_test2014_000000349439_result.jpg\n",
            "results/COCO_test2014_000000411444_result.jpg\n",
            "results/COCO_test2014_000000329547_result.jpg\n",
            "results/COCO_test2014_000000510557_result.jpg\n",
            "results/COCO_test2014_000000437560_result.jpg\n",
            "results/COCO_test2014_000000122201_result.jpg\n",
            "results/COCO_test2014_000000488669_result.jpg\n",
            "results/COCO_test2014_000000515528_result.jpg\n",
            "results/COCO_test2014_000000487969_result.jpg\n",
            "results/COCO_test2014_000000285931_result.jpg\n",
            "results/COCO_test2014_000000376287_result.jpg\n",
            "results/COCO_test2014_000000118869_result.jpg\n",
            "results/COCO_test2014_000000252321_result.jpg\n",
            "results/COCO_test2014_000000204425_result.jpg\n",
            "results/COCO_test2014_000000394107_result.jpg\n",
            "results/COCO_test2014_000000563152_result.jpg\n",
            "results/COCO_test2014_000000019046_result.jpg\n",
            "results/COCO_test2014_000000391257_result.jpg\n",
            "results/COCO_test2014_000000567269_result.jpg\n",
            "results/COCO_test2014_000000417235_result.jpg\n",
            "results/COCO_test2014_000000279232_result.jpg\n",
            "results/COCO_test2014_000000178240_result.jpg\n",
            "results/COCO_test2014_000000247550_result.jpg\n",
            "results/COCO_test2014_000000344670_result.jpg\n",
            "results/COCO_test2014_000000101609_result.jpg\n",
            "results/COCO_test2014_000000394825_result.jpg\n",
            "results/COCO_test2014_000000438853_result.jpg\n",
            "results/COCO_test2014_000000204330_result.jpg\n",
            "results/COCO_test2014_000000314875_result.jpg\n",
            "results/COCO_test2014_000000238328_result.jpg\n",
            "results/COCO_test2014_000000520711_result.jpg\n",
            "results/COCO_test2014_000000136684_result.jpg\n",
            "results/COCO_test2014_000000203568_result.jpg\n",
            "results/COCO_test2014_000000507089_result.jpg\n",
            "results/COCO_test2014_000000165690_result.jpg\n",
            "results/COCO_test2014_000000061917_result.jpg\n",
            "results/COCO_test2014_000000502403_result.jpg\n",
            "results/COCO_test2014_000000033424_result.jpg\n",
            "results/COCO_test2014_000000277994_result.jpg\n",
            "results/COCO_test2014_000000431231_result.jpg\n",
            "results/COCO_test2014_000000030268_result.jpg\n",
            "results/COCO_test2014_000000285541_result.jpg\n",
            "results/COCO_test2014_000000171462_result.jpg\n",
            "results/COCO_test2014_000000542136_result.jpg\n",
            "results/COCO_test2014_000000544357_result.jpg\n",
            "results/COCO_test2014_000000256115_result.jpg\n",
            "results/COCO_test2014_000000252650_result.jpg\n",
            "results/COCO_test2014_000000268069_result.jpg\n",
            "results/COCO_test2014_000000215054_result.jpg\n",
            "results/COCO_test2014_000000278534_result.jpg\n",
            "results/COCO_test2014_000000324029_result.jpg\n",
            "results/COCO_test2014_000000435702_result.jpg\n",
            "results/COCO_test2014_000000306092_result.jpg\n",
            "results/COCO_test2014_000000441867_result.jpg\n",
            "results/COCO_test2014_000000471509_result.jpg\n",
            "results/COCO_test2014_000000221426_result.jpg\n",
            "results/COCO_test2014_000000515813_result.jpg\n",
            "results/COCO_test2014_000000021189_result.jpg\n",
            "results/COCO_test2014_000000350258_result.jpg\n",
            "results/COCO_test2014_000000337357_result.jpg\n",
            "results/COCO_test2014_000000248960_result.jpg\n",
            "results/COCO_test2014_000000552713_result.jpg\n",
            "results/COCO_test2014_000000339649_result.jpg\n",
            "results/COCO_test2014_000000195242_result.jpg\n",
            "results/COCO_test2014_000000099556_result.jpg\n",
            "results/COCO_test2014_000000002343_result.jpg\n",
            "results/COCO_test2014_000000426977_result.jpg\n",
            "results/COCO_test2014_000000055863_result.jpg\n",
            "results/COCO_test2014_000000565038_result.jpg\n",
            "results/COCO_test2014_000000203335_result.jpg\n",
            "results/COCO_test2014_000000129906_result.jpg\n",
            "results/COCO_test2014_000000025375_result.jpg\n",
            "results/COCO_test2014_000000178140_result.jpg\n",
            "results/COCO_test2014_000000138566_result.jpg\n",
            "results/COCO_test2014_000000325726_result.jpg\n",
            "results/COCO_test2014_000000477292_result.jpg\n",
            "results/COCO_test2014_000000548675_result.jpg\n",
            "results/COCO_test2014_000000133405_result.jpg\n",
            "results/COCO_test2014_000000420391_result.jpg\n",
            "results/COCO_test2014_000000347364_result.jpg\n",
            "results/COCO_test2014_000000037804_result.jpg\n",
            "results/COCO_test2014_000000477393_result.jpg\n",
            "results/COCO_test2014_000000313279_result.jpg\n",
            "results/COCO_test2014_000000082637_result.jpg\n",
            "results/COCO_test2014_000000070377_result.jpg\n",
            "results/COCO_test2014_000000436006_result.jpg\n",
            "results/COCO_test2014_000000581862_result.jpg\n",
            "results/COCO_test2014_000000143676_result.jpg\n",
            "results/COCO_test2014_000000546259_result.jpg\n",
            "results/COCO_test2014_000000561356_result.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD4veQbibOFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1667ec47-55b2-49db-e4a4-401fd4913ca6"
      },
      "source": [
        "%cd\n",
        "%cd ../content/test/\n",
        "import os\n",
        "x = os.listdir('images')\n",
        "print(len(x))\n",
        "counter = 0\n",
        "\n",
        "for i in x:\n",
        "  if counter < 40600:\n",
        "    os.remove('images/'+i)\n",
        "  counter+=1\n",
        "    \n",
        "x = os.listdir('images') \n",
        "print(len(x))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/test\n",
            "40775\n",
            "175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_MQ62NGB0D3"
      },
      "source": [
        "%cd\n",
        "%cd ../content\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "\n",
        "image = plt.imread('gdrive/My Drive/withoutborder/edit0002.jpg')\n",
        "text = 'سلام'\n",
        "reshaped_text = arabic_reshaper.reshape(text)\n",
        "artext = get_display(reshaped_text)\n",
        "\n",
        "plt.title(artext)\n",
        "plt.imshow(image)\n",
        "plt.savefig('edit0001.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL6lx7CnqJFS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8eb4fc7d-4e27-4ad2-ddfc-7a578a14a4d0"
      },
      "source": [
        "#move image to test\n",
        "\n",
        "\n",
        "%cd\n",
        "!cp -r '../content/gdrive/My Drive/Final Project/TensorFlowImplementation/3.jpg' ../content/test/images\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBFA3SaaqY9"
      },
      "source": [
        "# Connecting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXFNHH50bojd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296u64vRvvC_"
      },
      "source": [
        "# Installing Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU_HmaP-vxq5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bb8d520-49ad-4d45-87bb-e631b25ac4b2"
      },
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrzwQG2Em4rC"
      },
      "source": [
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJwFyliVv1D5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp2N22_3jCR0"
      },
      "source": [
        "!pip install ipyparallel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cDWhWdRj4KQ"
      },
      "source": [
        "ipcluster nbextension enable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P81Vi99EFmrW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f6720134-b534-4bf2-f744-97913eca0b34"
      },
      "source": [
        "!pip install arabic_reshaper"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arabic_reshaper\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/b8/9f87dc2fc6c2e087e448db9e7f66ca4d68c22e9d49a95e5aad22d77c74f1/arabic_reshaper-2.0.15-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from arabic_reshaper) (41.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from arabic_reshaper) (0.16.0)\n",
            "Installing collected packages: arabic-reshaper\n",
            "Successfully installed arabic-reshaper-2.0.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guJ89OpnFp4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3b0be7c7-c605-4f88-f6ad-11bae45a93e9"
      },
      "source": [
        "!pip install python-bidi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-bidi\n",
            "  Downloading https://files.pythonhosted.org/packages/33/b0/f942d146a2f457233baaafd6bdf624eba8e0f665045b4abd69d1b62d097d/python_bidi-0.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-bidi) (1.12.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOYySU41xIon",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6173f211-fb00-4629-db40-b2445a8ea1f8"
      },
      "source": [
        "#install punkt from the list\n",
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsf8jwNh4BkP"
      },
      "source": [
        "# Loading Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYkgi2ce3-rd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b5e69734-53d7-45c0-9a98-2bc705e1594a"
      },
      "source": [
        "%cd\n",
        "%cd ../content\n",
        "!mkdir train \n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUAdI1gpnrEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "361a67d7-efbf-4bf0-b2cd-0f43b160c758"
      },
      "source": [
        "%cd  \n",
        "%cd ../content\n",
        "!mkdir val\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqsUVekrni09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4358af21-853e-4b61-b81a-825bdd2a5e85"
      },
      "source": [
        "%cd  \n",
        "%cd ../content\n",
        "!mkdir test\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbcw5HZ74J3c"
      },
      "source": [
        "Loading Train Dataset (Images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooYzRvG24G_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5dd703e6-c082-400e-dfd2-759c7d65c08f"
      },
      "source": [
        "%cd\n",
        "%cd ../content/train\n",
        "!wget -cq http://images.cocodataset.org/zips/train2014.zip\n",
        "!unzip -qq train2014.zip\n",
        "!mv train2014 images\n",
        "!rm train2014.zip\n",
        "!ls\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/train\n",
            "images\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-eib2Qf4PIJ"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Loading Validation Dataset (Images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyKqPmTz4Ho1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dc62f137-406b-41a5-d538-9eafbb70e9be"
      },
      "source": [
        "%cd\n",
        "%cd ../content/val\n",
        "!wget -cq http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip -qq val2014.zip\n",
        "!mv val2014 images\n",
        "!rm val2014.zip\n",
        "!ls\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/val\n",
            "images\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIUE7Wk54WzG"
      },
      "source": [
        "Loading Test Dataset (Images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y84hqvzu4YZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6ea4be00-3c27-40f4-bf8d-8729655f1be3"
      },
      "source": [
        "%cd\n",
        "%cd ../content/test\n",
        "!wget -cq http://images.cocodataset.org/zips/test2014.zip\n",
        "!unzip -qq test2014.zip\n",
        "!mv test2014 images\n",
        "!rm test2014.zip\n",
        "!ls\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/test\n",
            "archive.tar.gz\timages\tresults.csv\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c5x9c64c6K"
      },
      "source": [
        "Loading Annotations (English)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9lFifA4cRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "97987e36-7ece-4850-8b12-be1b531d5804"
      },
      "source": [
        "%cd\n",
        "%cd ../content/\n",
        "!wget -cq http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip -qq annotations_trainval2014.zip\n",
        "!rm annotations_trainval2014.zip\n",
        "%cd "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1grjsDEj7458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ff41ab57-408f-47ed-91c1-28bf7adef0b9"
      },
      "source": [
        "%cd \n",
        "!mv ../content/annotations/captions_train2014.json ../content/train\n",
        "!mv ../content/annotations/captions_val2014.json ../content/val\n",
        "%cd "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMomG28h3Cqp"
      },
      "source": [
        "Loading Annotations (Persian) , train + validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe5vokcsZwYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "177c5ec5-2f6e-43d5-ceeb-99c5f3af6b2c"
      },
      "source": [
        "%cd \n",
        "!cp '../content/gdrive/My Drive/Final Project/TensorFlowImplementation/captions_train2014_tran.json' ../content/train/captions_train2014.json\n",
        "!cp '../content/gdrive/My Drive/Final Project/TensorFlowImplementation/captions_val2014_tran.json' ../content/val/captions_val2014.json\n",
        "\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9pWq2Eg4kGG"
      },
      "source": [
        "# add some directories from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o14d4xqdahO"
      },
      "source": [
        "#downloading repository\n",
        "\n",
        "# %cd\n",
        "# %cd ../content\n",
        "# !wget -cq https://github.com/coldmanck/show-attend-and-tell/archive/master.zip\n",
        "# !unzip -qq master.zip\n",
        "# %cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnxKSNU4_fqc"
      },
      "source": [
        "#move some of the directories\n",
        "# %cd\n",
        "\n",
        "# !mv ../content/show-attend-and-tell-master/utils ../content/utils\n",
        "# !mv ../content/show-attend-and-tell-master/models ../content/models\n",
        "# !mv ../content/show-attend-and-tell-master/summary ../content/summary\n",
        "# !mv ../content/show-attend-and-tell-master/examples ../content/examples\n",
        "\n",
        "# %cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ClwtvvEDAvx"
      },
      "source": [
        "#deleting rest of the folder\n",
        "\n",
        "# %cd\n",
        "# %cd ../content\n",
        "# !rm -r show-attend-and-tell-master\n",
        "# !rm master.zip\n",
        "# %cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ah7X3dC8qv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "efe20af2-2732-4fec-e7a8-79930898acbf"
      },
      "source": [
        "%cd\n",
        "!cp -r '../content/gdrive/My Drive/Final Project/TensorFlowImplementation/utils' ../content/\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEYJvdSmDRnG"
      },
      "source": [
        "# base_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfJ8Pifkp0YX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "da89f32f-eb39-46a3-9ee8-895ad32e7aa8"
      },
      "source": [
        "%cd\n",
        "%cd ../content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcJydKmiDb9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78644b9a-63ed-4949-df56-34e806c0375a"
      },
      "source": [
        "%%writefile base_model.py\n",
        "\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import copy\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.nn import NN\n",
        "from utils.coco.coco import COCO\n",
        "from utils.coco.pycocoevalcap.eval import COCOEvalCap\n",
        "from utils.misc import ImageLoader, CaptionData, TopN\n",
        "\n",
        "class BaseModel(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.is_train = True if config.phase == 'train' else False\n",
        "        self.train_cnn = self.is_train and config.train_cnn\n",
        "        self.image_loader = ImageLoader('./utils/ilsvrc_2012_mean.npy')\n",
        "        self.image_shape = [224, 224, 3]\n",
        "        self.nn = NN(config)\n",
        "        self.global_step = tf.Variable(0,\n",
        "                                       name = 'global_step',\n",
        "                                       trainable = False)\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def train(self, sess, train_data):\n",
        "        \"\"\" Train the model using the COCO train2014 data. \"\"\"\n",
        "        print(\"Training the model...\")\n",
        "        config = self.config\n",
        "\n",
        "        if not os.path.exists(config.summary_dir):\n",
        "            os.mkdir(config.summary_dir)\n",
        "        train_writer = tf.summary.FileWriter(config.summary_dir,\n",
        "                                             sess.graph)\n",
        "\n",
        "        for _ in tqdm(list(range(config.num_epochs)), desc='epoch'):\n",
        "            for _ in tqdm(list(range(train_data.num_batches)), desc='batch'):\n",
        "                batch = train_data.next_batch()\n",
        "                image_files, sentences, masks = batch\n",
        "                images = self.image_loader.load_images(image_files)\n",
        "                feed_dict = {self.images: images,\n",
        "                             self.sentences: sentences,\n",
        "                             self.masks: masks}\n",
        "                _, summary, global_step = sess.run([self.opt_op,\n",
        "                                                    self.summary,\n",
        "                                                    self.global_step],\n",
        "                                                    feed_dict=feed_dict)\n",
        "                if (global_step + 1) % config.save_period == 0:\n",
        "                    self.save()\n",
        "                train_writer.add_summary(summary, global_step)\n",
        "            train_data.reset()\n",
        "\n",
        "        self.save()\n",
        "        train_writer.close()\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "    def eval(self, sess, eval_gt_coco, eval_data, vocabulary):\n",
        "        \"\"\" Evaluate the model using the COCO val2014 data. \"\"\"\n",
        "        print(\"Evaluating the model ...\")\n",
        "        config = self.config\n",
        "\n",
        "        results = []\n",
        "        print('config.eval_result_dir:', config.eval_result_dir)\n",
        "        if not os.path.exists(config.eval_result_dir):\n",
        "            os.mkdir(config.eval_result_dir)\n",
        "\n",
        "        # Generate the captions for the images\n",
        "        idx = 0\n",
        "        for k in tqdm(list(range(eval_data.num_batches)), desc='batch'):\n",
        "            batch = eval_data.next_batch()\n",
        "            caption_data = self.beam_search(sess, batch, vocabulary)\n",
        "\n",
        "            fake_cnt = 0 if k<eval_data.num_batches-1 \\\n",
        "                         else eval_data.fake_count\n",
        "            for l in range(eval_data.batch_size-fake_cnt):\n",
        "                word_idxs = caption_data[l][0].sentence\n",
        "                score = caption_data[l][0].score\n",
        "                caption = vocabulary.get_sentence(word_idxs)\n",
        "                results.append({'image_id': eval_data.image_ids[idx].item(),\n",
        "                                'caption': caption})\n",
        "                idx += 1\n",
        "\n",
        "                # Save the result in an image file, if requested\n",
        "                if config.save_eval_result_as_image:\n",
        "                    image_file = batch[l]\n",
        "                    image_name = image_file.split(os.sep)[-1]\n",
        "                    image_name = os.path.splitext(image_name)[0]\n",
        "                    img = plt.imread(image_file)\n",
        "                    plt.imshow(img)\n",
        "                    plt.axis('off')\n",
        "                    plt.title(caption)\n",
        "                    plt.savefig(os.path.join(config.eval_result_dir,\n",
        "                                             image_name+'_result.jpg'))\n",
        "\n",
        "        fp = open(config.eval_result_file, 'w')\n",
        "        json.dump(results, fp)\n",
        "        fp.close()\n",
        "\n",
        "        # Evaluate these captions\n",
        "        eval_result_coco = eval_gt_coco.loadRes(config.eval_result_file)\n",
        "        scorer = COCOEvalCap(eval_gt_coco, eval_result_coco)\n",
        "        scorer.evaluate()\n",
        "        print(\"Evaluation complete.\")\n",
        "\n",
        "    def test(self, sess, test_data, vocabulary):\n",
        "        \"\"\" Test the model using any given images. \"\"\"\n",
        "        print(\"Testing the model ...\")\n",
        "        config = self.config\n",
        "\n",
        "        if not os.path.exists(config.test_result_dir):\n",
        "            os.mkdir(config.test_result_dir)\n",
        "\n",
        "        captions = []\n",
        "        scores = []\n",
        "\n",
        "        # Generate the captions for the images\n",
        "        for k in tqdm(list(range(test_data.num_batches)), desc='path'):\n",
        "            batch = test_data.next_batch()\n",
        "            caption_data = self.beam_search(sess, batch, vocabulary)\n",
        "\n",
        "            fake_cnt = 0 if k<test_data.num_batches-1 \\\n",
        "                         else test_data.fake_count\n",
        "            for l in range(test_data.batch_size-fake_cnt):\n",
        "                word_idxs = caption_data[l][0].sentence\n",
        "                score = caption_data[l][0].score\n",
        "                caption = vocabulary.get_sentence(word_idxs)\n",
        "                captions.append(caption)\n",
        "                scores.append(score)\n",
        "\n",
        "                # Save the result in an image file\n",
        "                image_file = batch[l]\n",
        "                image_name = image_file.split(os.sep)[-1]\n",
        "                image_name = os.path.splitext(image_name)[0]\n",
        "                img = plt.imread(image_file)\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                if config.language =='fa':\n",
        "                  reshaped_text = arabic_reshaper.reshape(caption)\n",
        "                  arcaption = get_display(reshaped_text)\n",
        "                  plt.title(arcaption)\n",
        "                else: \n",
        "                  plt.title(caption)\n",
        "                  \n",
        "                plt.savefig(os.path.join(config.test_result_dir,\n",
        "                                         image_name+'_result.jpg'))\n",
        "\n",
        "        # Save the captions to a file\n",
        "        results = pd.DataFrame({'image_files':test_data.image_files,\n",
        "                                'caption':captions,\n",
        "                                'prob':scores})\n",
        "        results.to_csv(config.test_result_file)\n",
        "        print(\"Testing complete.\")\n",
        "\n",
        "    def beam_search(self, sess, image_files, vocabulary):\n",
        "        \"\"\"Use beam search to generate the captions for a batch of images.\"\"\"\n",
        "        # Feed in the images to get the contexts and the initial LSTM states\n",
        "        config = self.config\n",
        "        images = self.image_loader.load_images(image_files)\n",
        "        contexts, initial_memory, initial_output = sess.run(\n",
        "            [self.conv_feats, self.initial_memory, self.initial_output],\n",
        "            feed_dict = {self.images: images})\n",
        "\n",
        "        partial_caption_data = []\n",
        "        complete_caption_data = []\n",
        "        for k in range(config.batch_size):\n",
        "            initial_beam = CaptionData(sentence = [],\n",
        "                                       memory = initial_memory[k],\n",
        "                                       output = initial_output[k],\n",
        "                                       score = 1.0)\n",
        "            partial_caption_data.append(TopN(config.beam_size))\n",
        "            partial_caption_data[-1].push(initial_beam)\n",
        "            complete_caption_data.append(TopN(config.beam_size))\n",
        "\n",
        "        # Run beam search\n",
        "        for idx in range(config.max_caption_length):\n",
        "            partial_caption_data_lists = []\n",
        "            for k in range(config.batch_size):\n",
        "                data = partial_caption_data[k].extract()\n",
        "                partial_caption_data_lists.append(data)\n",
        "                partial_caption_data[k].reset()\n",
        "\n",
        "            num_steps = 1 if idx == 0 else config.beam_size\n",
        "            for b in range(num_steps):\n",
        "                if idx == 0:\n",
        "                    last_word = np.zeros((config.batch_size), np.int32)\n",
        "                else:\n",
        "                    last_word = np.array([pcl[b].sentence[-1]\n",
        "                                        for pcl in partial_caption_data_lists],\n",
        "                                        np.int32)\n",
        "\n",
        "                last_memory = np.array([pcl[b].memory\n",
        "                                        for pcl in partial_caption_data_lists],\n",
        "                                        np.float32)\n",
        "                last_output = np.array([pcl[b].output\n",
        "                                        for pcl in partial_caption_data_lists],\n",
        "                                        np.float32)\n",
        "\n",
        "                memory, output, scores = sess.run(\n",
        "                    [self.memory, self.output, self.probs],\n",
        "                    feed_dict = {self.contexts: contexts,\n",
        "                                 self.last_word: last_word,\n",
        "                                 self.last_memory: last_memory,\n",
        "                                 self.last_output: last_output})\n",
        "\n",
        "                # Find the beam_size most probable next words\n",
        "                for k in range(config.batch_size):\n",
        "                    caption_data = partial_caption_data_lists[k][b]\n",
        "                    words_and_scores = list(enumerate(scores[k]))\n",
        "                    words_and_scores.sort(key=lambda x: -x[1])\n",
        "                    words_and_scores = words_and_scores[0:config.beam_size+1]\n",
        "\n",
        "                    # Append each of these words to the current partial caption\n",
        "                    for w, s in words_and_scores:\n",
        "                        sentence = caption_data.sentence + [w]\n",
        "                        score = caption_data.score * s\n",
        "                        beam = CaptionData(sentence,\n",
        "                                           memory[k],\n",
        "                                           output[k],\n",
        "                                           score)\n",
        "                        if vocabulary.words[w] == '.':\n",
        "                            complete_caption_data[k].push(beam)\n",
        "                        else:\n",
        "                            partial_caption_data[k].push(beam)\n",
        "\n",
        "        results = []\n",
        "        for k in range(config.batch_size):\n",
        "            if complete_caption_data[k].size() == 0:\n",
        "                complete_caption_data[k] = partial_caption_data[k]\n",
        "            results.append(complete_caption_data[k].extract(sort=True))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" Save the model. \"\"\"\n",
        "        config = self.config\n",
        "        data = {v.name: v.eval() for v in tf.global_variables()}\n",
        "        save_path = os.path.join(config.save_dir, str(self.global_step.eval()))\n",
        "\n",
        "        print((\" Saving the model to %s...\" % (save_path+\".npy\")))\n",
        "        np.save(save_path, data)\n",
        "        info_file = open(os.path.join(config.save_dir, \"config.pickle\"), \"wb\")\n",
        "        config_ = copy.copy(config)\n",
        "        config_.global_step = self.global_step.eval()\n",
        "        pickle.dump(config_, info_file)\n",
        "        info_file.close()\n",
        "        \n",
        "        if config.only_last_model :\n",
        "          self.delete_older_checkpoints()\n",
        "        print(\"Model saved.\")\n",
        "        \n",
        "    def delete_older_checkpoints(self):\n",
        "      config = self.config\n",
        "      print(\"Deleting models older than %s ...\" % (str(self.global_step.eval())))\n",
        "      checkpoints_list = os.listdir(config.save_dir)\n",
        "      for i in checkpoints_list:  \n",
        "        if '.npy' in i:\n",
        "          checkpointnumber = i.split(\".\")[0]\n",
        "          if self.global_step.eval() > int(checkpointnumber):\n",
        "            os.remove(config.save_dir+\"/\"+i)\n",
        "      print(\"Older models deleted\")\n",
        "        \n",
        "    def load(self, sess, model_file=None):\n",
        "        \"\"\" Load the model. \"\"\"\n",
        "        config = self.config\n",
        "        if model_file is not None:\n",
        "            save_path = model_file\n",
        "        else:\n",
        "            info_path = os.path.join(config.save_dir, \"config.pickle\")\n",
        "            info_file = open(info_path, \"rb\")\n",
        "            config = pickle.load(info_file)\n",
        "            global_step = config.global_step\n",
        "            info_file.close()\n",
        "            save_path = os.path.join(config.save_dir,\n",
        "                                     str(global_step)+\".npy\")\n",
        "\n",
        "        print(\"Loading the model from %s...\" %save_path)\n",
        "        data_dict = np.load(save_path, encoding='latin1').item()\n",
        "        count = 0\n",
        "        for v in tqdm(tf.global_variables()):\n",
        "            if v.name in data_dict.keys():\n",
        "                sess.run(v.assign(data_dict[v.name]))\n",
        "                count += 1\n",
        "        print(\"%d tensors loaded.\" %count)\n",
        "\n",
        "    def load_cnn(self, session, data_path, ignore_missing=True):\n",
        "        \"\"\" Load a pretrained CNN model. \"\"\"\n",
        "        print(\"Loading the CNN from %s...\" %data_path)\n",
        "        # import pdb; pdb.set_trace()\n",
        "        import os;\n",
        "        data_path = data_path.strip()\n",
        "        data_dict = np.load(os.getcwd() + '/' + data_path, encoding='latin1').item()\n",
        "        count = 0\n",
        "        for op_name in tqdm(data_dict):\n",
        "            with tf.variable_scope(op_name, reuse = True):\n",
        "                for param_name, data in data_dict[op_name].items():\n",
        "                    try:\n",
        "                        var = tf.get_variable(param_name)\n",
        "                        session.run(var.assign(data))\n",
        "                        count += 1\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "        print(\"%d tensors loaded.\" %count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing base_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsLY9FFQDkG1"
      },
      "source": [
        "# config.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RsQvtknqM1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6e0b9e0-4a54-4fc0-b2c4-b65a1a8dd003"
      },
      "source": [
        "%cd\n",
        "%cd ../content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3rzj-zkDnGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "364c428e-8ac0-4048-b54e-75b2cb776503"
      },
      "source": [
        "%%writefile config.py\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\" Wrapper class for various (hyper)parameters. \"\"\"\n",
        "    def __init__(self):\n",
        "        # about the model architecture\n",
        "        self.cnn = 'vgg16'               # 'vgg16' or 'resnet50'\n",
        "        self.max_caption_length = 20\n",
        "        self.dim_embedding = 512\n",
        "        self.num_lstm_units = 512\n",
        "        self.num_initalize_layers = 2    # 1 or 2\n",
        "        self.dim_initalize_layer = 512\n",
        "        self.num_attend_layers = 2       # 1 or 2\n",
        "        self.dim_attend_layer = 512\n",
        "        self.num_decode_layers = 2       # 1 or 2\n",
        "        self.dim_decode_layer = 1024\n",
        "\n",
        "        # about the weight initialization and regularization\n",
        "        self.fc_kernel_initializer_scale = 0.08\n",
        "        self.fc_kernel_regularizer_scale = 1e-4\n",
        "        self.fc_activity_regularizer_scale = 0.0\n",
        "        self.conv_kernel_regularizer_scale = 1e-4\n",
        "        self.conv_activity_regularizer_scale = 0.0\n",
        "        self.fc_drop_rate = 0.5\n",
        "        self.lstm_drop_rate = 0.3\n",
        "        self.attention_loss_factor = 0.01\n",
        "\n",
        "        # about the optimization\n",
        "        self.num_epochs = 100\n",
        "        self.batch_size = 32\n",
        "        self.optimizer = 'Adam'    # 'Adam', 'RMSProp', 'Momentum' or 'SGD'\n",
        "        self.initial_learning_rate = 0.0001\n",
        "        self.learning_rate_decay_factor = 1.0\n",
        "        self.num_steps_per_decay = 100000\n",
        "        self.clip_gradients = 5.0\n",
        "        self.momentum = 0.0\n",
        "        self.use_nesterov = True\n",
        "        self.decay = 0.9\n",
        "        self.centered = True\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "        # about the saver\n",
        "        self.save_period = 1000\n",
        "#         self.save_dir = 'gdrive/My Drive/Final Project/TensorFlowImplementation/Farsi Models'\n",
        "#         self.save_dir = './models/'\n",
        "        self.summary_dir = './summary/'\n",
        "        self.only_last_model = True\n",
        "\n",
        "        # about the vocabulary\n",
        "        self.vocabulary_file = './vocabulary.csv'\n",
        "        self.vocabulary_size = 5000\n",
        "\n",
        "        # about the training\n",
        "        self.train_image_dir = './train/images/'\n",
        "        self.train_caption_file = './train/captions_train2014.json'\n",
        "        self.temp_annotation_file = './train/anns.csv'\n",
        "        self.temp_data_file = './train/data.npy'\n",
        "\n",
        "        # about the evaluation\n",
        "        self.eval_image_dir = './val/images/'\n",
        "        self.eval_caption_file = './val/captions_val2014.json'\n",
        "        self.eval_result_dir = './val/results/'\n",
        "        self.eval_result_file = './val/results.json'\n",
        "        self.save_eval_result_as_image = False\n",
        "\n",
        "        # about the testing\n",
        "        self.test_image_dir = './test/images/'\n",
        "        self.test_result_dir = './test/results/'\n",
        "        self.test_result_file = './test/results.csv'\n",
        "        \n",
        "        #about the language\n",
        "#         self.language = 'fa'\n",
        "        self.language = 'en'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing config.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQo128TYDqrc"
      },
      "source": [
        "# dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWuHY1w1qPYN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ced0d95f-1554-48c5-a7b5-fb7e35c5470c"
      },
      "source": [
        "%cd\n",
        "%cd ../content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WalLRtnJDtpZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce7f3e02-a641-4c3a-a1d4-7afc513efc08"
      },
      "source": [
        "%%writefile dataset.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.coco.coco import COCO\n",
        "from utils.vocabulary import Vocabulary\n",
        "\n",
        "class DataSet(object):\n",
        "    def __init__(self,\n",
        "                 image_ids,\n",
        "                 image_files,\n",
        "                 batch_size,\n",
        "                 word_idxs=None,\n",
        "                 masks=None,\n",
        "                 is_train=False,\n",
        "                 shuffle=False):\n",
        "        self.image_ids = np.array(image_ids)\n",
        "        self.image_files = np.array(image_files)\n",
        "        self.word_idxs = np.array(word_idxs)\n",
        "        self.masks = np.array(masks)\n",
        "        self.batch_size = batch_size\n",
        "        self.is_train = is_train\n",
        "        self.shuffle = shuffle\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\" Setup the dataset. \"\"\"\n",
        "        self.count = len(self.image_ids)\n",
        "        self.num_batches = int(np.ceil(self.count * 1.0 / self.batch_size))\n",
        "        self.fake_count = self.num_batches * self.batch_size - self.count\n",
        "        self.idxs = list(range(self.count))\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Reset the dataset. \"\"\"\n",
        "        self.current_idx = 0\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.idxs)\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\" Fetch the next batch. \"\"\"\n",
        "        assert self.has_next_batch()\n",
        "\n",
        "        if self.has_full_next_batch():\n",
        "            start, end = self.current_idx, \\\n",
        "                         self.current_idx + self.batch_size\n",
        "            current_idxs = self.idxs[start:end]\n",
        "        else:\n",
        "            start, end = self.current_idx, self.count\n",
        "            current_idxs = self.idxs[start:end] + \\\n",
        "                           list(np.random.choice(self.count, self.fake_count))\n",
        "\n",
        "        image_files = self.image_files[current_idxs]\n",
        "        if self.is_train:\n",
        "            word_idxs = self.word_idxs[current_idxs]\n",
        "            masks = self.masks[current_idxs]\n",
        "            self.current_idx += self.batch_size\n",
        "            return image_files, word_idxs, masks\n",
        "        else:\n",
        "            self.current_idx += self.batch_size\n",
        "            return image_files\n",
        "\n",
        "    def has_next_batch(self):\n",
        "        \"\"\" Determine whether there is a batch left. \"\"\"\n",
        "        return self.current_idx < self.count\n",
        "\n",
        "    def has_full_next_batch(self):\n",
        "        \"\"\" Determine whether there is a full batch left. \"\"\"\n",
        "        return self.current_idx + self.batch_size <= self.count\n",
        "\n",
        "def prepare_train_data(config):\n",
        "    \"\"\" Prepare the data for training the model. \"\"\"\n",
        "    coco = COCO(config.train_caption_file) # address of the farsi caption file ***************\n",
        "    coco.filter_by_cap_len(config.max_caption_length)\n",
        "\n",
        "    print(\"Building the vocabulary...\")\n",
        "    vocabulary = Vocabulary(config.vocabulary_size)\n",
        "    if not os.path.exists(config.vocabulary_file):\n",
        "        vocabulary.build(coco.all_captions())\n",
        "        vocabulary.save(config.vocabulary_file)\n",
        "    else:\n",
        "      # add a print to this part after upload a csv file manually for vocab ***************\n",
        "        vocabulary.load(config.vocabulary_file)\n",
        "    print(\"Vocabulary built.\")\n",
        "    print(\"Number of words = %d\" %(vocabulary.size))\n",
        "\n",
        "    coco.filter_by_words(set(vocabulary.words))\n",
        "\n",
        "    print(\"Processing the captions...\")\n",
        "    if not os.path.exists(config.temp_annotation_file):\n",
        "        captions = [coco.anns[ann_id]['caption'] for ann_id in coco.anns]\n",
        "        image_ids = [coco.anns[ann_id]['image_id'] for ann_id in coco.anns]\n",
        "        image_files = [os.path.join(config.train_image_dir,\n",
        "                                    coco.imgs[image_id]['file_name'])\n",
        "                                    for image_id in image_ids]\n",
        "        annotations = pd.DataFrame({'image_id': image_ids,\n",
        "                                    'image_file': image_files,\n",
        "                                    'caption': captions})\n",
        "        annotations.to_csv(config.temp_annotation_file)\n",
        "    else:\n",
        "        annotations = pd.read_csv(config.temp_annotation_file)\n",
        "        captions = annotations['caption'].values\n",
        "        image_ids = annotations['image_id'].values\n",
        "        image_files = annotations['image_file'].values\n",
        "\n",
        "    if not os.path.exists(config.temp_data_file):\n",
        "        word_idxs = []\n",
        "        masks = []\n",
        "        for caption in tqdm(captions):\n",
        "            current_word_idxs_ = vocabulary.process_sentence(caption)\n",
        "            current_num_words = len(current_word_idxs_)\n",
        "            current_word_idxs = np.zeros(config.max_caption_length,\n",
        "                                         dtype = np.int32)\n",
        "            current_masks = np.zeros(config.max_caption_length)\n",
        "            current_word_idxs[:current_num_words] = np.array(current_word_idxs_)\n",
        "            current_masks[:current_num_words] = 1.0\n",
        "            word_idxs.append(current_word_idxs)\n",
        "            masks.append(current_masks)\n",
        "        word_idxs = np.array(word_idxs)\n",
        "        masks = np.array(masks)\n",
        "        data = {'word_idxs': word_idxs, 'masks': masks}\n",
        "        np.save(config.temp_data_file, data)\n",
        "    else:\n",
        "        data = np.load(config.temp_data_file, encoding='latin1').item()\n",
        "        word_idxs = data['word_idxs']\n",
        "        masks = data['masks']\n",
        "    print(\"Captions processed.\")\n",
        "    print(\"Number of captions = %d\" %(len(captions)))\n",
        "\n",
        "    print(\"Building the dataset...\")\n",
        "    dataset = DataSet(image_ids,\n",
        "                      image_files,\n",
        "                      config.batch_size,\n",
        "                      word_idxs,\n",
        "                      masks,\n",
        "                      True,\n",
        "                      True)\n",
        "    print(\"Dataset built.\")\n",
        "    return dataset\n",
        "\n",
        "def prepare_eval_data(config):\n",
        "    \"\"\" Prepare the data for evaluating the model. \"\"\"\n",
        "    coco = COCO(config.eval_caption_file)\n",
        "    image_ids = list(coco.imgs.keys())\n",
        "    image_files = [os.path.join(config.eval_image_dir,\n",
        "                                coco.imgs[image_id]['file_name'])\n",
        "                                for image_id in image_ids]\n",
        "\n",
        "    print(\"Building the vocabulary...\")\n",
        "    if os.path.exists(config.vocabulary_file):\n",
        "        vocabulary = Vocabulary(config.vocabulary_size,\n",
        "                                config.vocabulary_file)\n",
        "    else:\n",
        "        vocabulary = build_vocabulary(config)\n",
        "    print(\"Vocabulary built.\")\n",
        "    print(\"Number of words = %d\" %(vocabulary.size))\n",
        "\n",
        "    print(\"Building the dataset...\")\n",
        "    dataset = DataSet(image_ids, image_files, config.batch_size)\n",
        "    print(\"Dataset built.\")\n",
        "    return coco, dataset, vocabulary\n",
        "\n",
        "def prepare_test_data(config):\n",
        "    \"\"\" Prepare the data for testing the model. \"\"\"\n",
        "    files = os.listdir(config.test_image_dir)\n",
        "    image_files = [os.path.join(config.test_image_dir, f) for f in files\n",
        "        if f.lower().endswith('.jpg') or f.lower().endswith('.jpeg')]\n",
        "    image_ids = list(range(len(image_files)))\n",
        "\n",
        "    print(\"Building the vocabulary...\")\n",
        "    if os.path.exists(config.vocabulary_file):\n",
        "        vocabulary = Vocabulary(config.vocabulary_size,\n",
        "                                config.vocabulary_file)\n",
        "    else:\n",
        "        vocabulary = build_vocabulary(config)\n",
        "    print(\"Vocabulary built.\")\n",
        "    print(\"Number of words = %d\" %(vocabulary.size))\n",
        "\n",
        "    print(\"Building the dataset...\")\n",
        "    dataset = DataSet(image_ids, image_files, config.batch_size)\n",
        "    print(\"Dataset built.\")\n",
        "    return dataset, vocabulary\n",
        "\n",
        "def build_vocabulary(config):\n",
        "    \"\"\" Build the vocabulary from the training data and save it to a file. \"\"\"\n",
        "    coco = COCO(config.train_caption_file)\n",
        "    coco.filter_by_cap_len(config.max_caption_length)\n",
        "\n",
        "    vocabulary = Vocabulary(config.vocabulary_size)\n",
        "    vocabulary.build(coco.all_captions())\n",
        "    vocabulary.save(config.vocabulary_file)\n",
        "    return vocabulary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyB9RMpVD2S0"
      },
      "source": [
        "# main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ1DBJRxqRA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b569ec9-06d0-4245-a361-3070112fb1b5"
      },
      "source": [
        "%cd\n",
        "%cd ../content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veE6vaIWD3fg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "024e7566-fec9-49b3-f405-797431de0df7"
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "\n",
        "#!/usr/bin/python\n",
        "import tensorflow as tf\n",
        "\n",
        "from config import Config\n",
        "from model import CaptionGenerator\n",
        "from dataset import prepare_train_data, prepare_eval_data, prepare_test_data\n",
        "\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "tf.flags.DEFINE_string('phase', 'train',\n",
        "                       'The phase can be train, eval or test')\n",
        "\n",
        "tf.flags.DEFINE_boolean('load', False,\n",
        "                        'Turn on to load a pretrained model from either \\\n",
        "                        the latest checkpoint or a specified file')\n",
        "\n",
        "tf.flags.DEFINE_string('model_file', None,\n",
        "                       'If sepcified, load a pretrained model from this file')\n",
        "\n",
        "tf.flags.DEFINE_boolean('load_cnn', False,\n",
        "                        'Turn on to load a pretrained CNN model')\n",
        "\n",
        "tf.flags.DEFINE_string('cnn_model_file', './vgg16_no_fc.npy',\n",
        "                       'The file containing a pretrained CNN model')\n",
        "\n",
        "tf.flags.DEFINE_boolean('train_cnn', False,\n",
        "                        'Turn on to train both CNN and RNN. \\\n",
        "                         Otherwise, only RNN is trained')\n",
        "\n",
        "tf.flags.DEFINE_integer('beam_size', 3,\n",
        "                        'The size of beam search for caption generation')\n",
        "\n",
        "def main(argv):\n",
        "    config = Config()\n",
        "    config.phase = FLAGS.phase\n",
        "    config.train_cnn = FLAGS.train_cnn\n",
        "    config.beam_size = FLAGS.beam_size\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        if FLAGS.phase == 'train':\n",
        "            # training phase\n",
        "            data = prepare_train_data(config)\n",
        "            model = CaptionGenerator(config)\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            if FLAGS.load:\n",
        "                model.load(sess, FLAGS.model_file)\n",
        "            if FLAGS.load_cnn:\n",
        "                model.load_cnn(sess, FLAGS.cnn_model_file)\n",
        "            tf.get_default_graph().finalize()\n",
        "            model.train(sess, data)\n",
        "\n",
        "        elif FLAGS.phase == 'eval':\n",
        "            # evaluation phase\n",
        "            coco, data, vocabulary = prepare_eval_data(config)\n",
        "            model = CaptionGenerator(config)\n",
        "            model.load(sess, FLAGS.model_file)\n",
        "            tf.get_default_graph().finalize()\n",
        "            model.eval(sess, coco, data, vocabulary)\n",
        "\n",
        "        else:\n",
        "            # testing phase\n",
        "            data, vocabulary = prepare_test_data(config)\n",
        "            model = CaptionGenerator(config)\n",
        "            model.load(sess, FLAGS.model_file)\n",
        "            tf.get_default_graph().finalize()\n",
        "            model.test(sess, data, vocabulary)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nozw9_4D8YK"
      },
      "source": [
        "# model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hOJcLZNqS_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "06e858c5-0cde-45e8-dc8d-fe99cc03c872"
      },
      "source": [
        "%cd\n",
        "%cd ../content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SNbx2fuD9vn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6216dd6-a858-475b-d4ba-c5bde126a35d"
      },
      "source": [
        "%%writefile model.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from base_model import BaseModel\n",
        "\n",
        "class CaptionGenerator(BaseModel):\n",
        "    def build(self):\n",
        "        \"\"\" Build the model. \"\"\"\n",
        "        self.build_cnn()\n",
        "        self.build_rnn()\n",
        "        if self.is_train:\n",
        "            self.build_optimizer()\n",
        "            self.build_summary()\n",
        "\n",
        "    def build_cnn(self):\n",
        "        \"\"\" Build the CNN. \"\"\"\n",
        "        print(\"Building the CNN...\")\n",
        "        if self.config.cnn == 'vgg16':\n",
        "            self.build_vgg16()\n",
        "        else:\n",
        "            self.build_resnet50()\n",
        "        print(\"CNN built.\")\n",
        "\n",
        "    def build_vgg16(self):\n",
        "        \"\"\" Build the VGG16 net. \"\"\"\n",
        "        config = self.config\n",
        "\n",
        "        images = tf.placeholder(\n",
        "            dtype = tf.float32,\n",
        "            shape = [config.batch_size] + self.image_shape)\n",
        "\n",
        "        conv1_1_feats = self.nn.conv2d(images, 64, name = 'conv1_1')\n",
        "        conv1_2_feats = self.nn.conv2d(conv1_1_feats, 64, name = 'conv1_2')\n",
        "        pool1_feats = self.nn.max_pool2d(conv1_2_feats, name = 'pool1')\n",
        "\n",
        "        conv2_1_feats = self.nn.conv2d(pool1_feats, 128, name = 'conv2_1')\n",
        "        conv2_2_feats = self.nn.conv2d(conv2_1_feats, 128, name = 'conv2_2')\n",
        "        pool2_feats = self.nn.max_pool2d(conv2_2_feats, name = 'pool2')\n",
        "\n",
        "        conv3_1_feats = self.nn.conv2d(pool2_feats, 256, name = 'conv3_1')\n",
        "        conv3_2_feats = self.nn.conv2d(conv3_1_feats, 256, name = 'conv3_2')\n",
        "        conv3_3_feats = self.nn.conv2d(conv3_2_feats, 256, name = 'conv3_3')\n",
        "        pool3_feats = self.nn.max_pool2d(conv3_3_feats, name = 'pool3')\n",
        "\n",
        "        conv4_1_feats = self.nn.conv2d(pool3_feats, 512, name = 'conv4_1')\n",
        "        conv4_2_feats = self.nn.conv2d(conv4_1_feats, 512, name = 'conv4_2')\n",
        "        conv4_3_feats = self.nn.conv2d(conv4_2_feats, 512, name = 'conv4_3')\n",
        "        pool4_feats = self.nn.max_pool2d(conv4_3_feats, name = 'pool4')\n",
        "\n",
        "        conv5_1_feats = self.nn.conv2d(pool4_feats, 512, name = 'conv5_1')\n",
        "        conv5_2_feats = self.nn.conv2d(conv5_1_feats, 512, name = 'conv5_2')\n",
        "        conv5_3_feats = self.nn.conv2d(conv5_2_feats, 512, name = 'conv5_3')\n",
        "\n",
        "        reshaped_conv5_3_feats = tf.reshape(conv5_3_feats,\n",
        "                                            [config.batch_size, 196, 512])\n",
        "\n",
        "        self.conv_feats = reshaped_conv5_3_feats\n",
        "        self.num_ctx = 196\n",
        "        self.dim_ctx = 512\n",
        "        self.images = images\n",
        "\n",
        "    def build_resnet50(self):\n",
        "        \"\"\" Build the ResNet50. \"\"\"\n",
        "        config = self.config\n",
        "\n",
        "        images = tf.placeholder(\n",
        "            dtype = tf.float32,\n",
        "            shape = [config.batch_size] + self.image_shape)\n",
        "\n",
        "        conv1_feats = self.nn.conv2d(images,\n",
        "                                  filters = 64,\n",
        "                                  kernel_size = (7, 7),\n",
        "                                  strides = (2, 2),\n",
        "                                  activation = None,\n",
        "                                  name = 'conv1')\n",
        "        conv1_feats = self.nn.batch_norm(conv1_feats, 'bn_conv1')\n",
        "        conv1_feats = tf.nn.relu(conv1_feats)\n",
        "        pool1_feats = self.nn.max_pool2d(conv1_feats,\n",
        "                                      pool_size = (3, 3),\n",
        "                                      strides = (2, 2),\n",
        "                                      name = 'pool1')\n",
        "\n",
        "        res2a_feats = self.resnet_block(pool1_feats, 'res2a', 'bn2a', 64, 1)\n",
        "        res2b_feats = self.resnet_block2(res2a_feats, 'res2b', 'bn2b', 64)\n",
        "        res2c_feats = self.resnet_block2(res2b_feats, 'res2c', 'bn2c', 64)\n",
        "\n",
        "        res3a_feats = self.resnet_block(res2c_feats, 'res3a', 'bn3a', 128)\n",
        "        res3b_feats = self.resnet_block2(res3a_feats, 'res3b', 'bn3b', 128)\n",
        "        res3c_feats = self.resnet_block2(res3b_feats, 'res3c', 'bn3c', 128)\n",
        "        res3d_feats = self.resnet_block2(res3c_feats, 'res3d', 'bn3d', 128)\n",
        "\n",
        "        res4a_feats = self.resnet_block(res3d_feats, 'res4a', 'bn4a', 256)\n",
        "        res4b_feats = self.resnet_block2(res4a_feats, 'res4b', 'bn4b', 256)\n",
        "        res4c_feats = self.resnet_block2(res4b_feats, 'res4c', 'bn4c', 256)\n",
        "        res4d_feats = self.resnet_block2(res4c_feats, 'res4d', 'bn4d', 256)\n",
        "        res4e_feats = self.resnet_block2(res4d_feats, 'res4e', 'bn4e', 256)\n",
        "        res4f_feats = self.resnet_block2(res4e_feats, 'res4f', 'bn4f', 256)\n",
        "\n",
        "        res5a_feats = self.resnet_block(res4f_feats, 'res5a', 'bn5a', 512)\n",
        "        res5b_feats = self.resnet_block2(res5a_feats, 'res5b', 'bn5b', 512)\n",
        "        res5c_feats = self.resnet_block2(res5b_feats, 'res5c', 'bn5c', 512)\n",
        "\n",
        "        reshaped_res5c_feats = tf.reshape(res5c_feats,\n",
        "                                         [config.batch_size, 49, 2048])\n",
        "\n",
        "        self.conv_feats = reshaped_res5c_feats\n",
        "        self.num_ctx = 49\n",
        "        self.dim_ctx = 2048\n",
        "        self.images = images\n",
        "\n",
        "    def resnet_block(self, inputs, name1, name2, c, s=2):\n",
        "        \"\"\" A basic block of ResNet. \"\"\"\n",
        "        branch1_feats = self.nn.conv2d(inputs,\n",
        "                                    filters = 4*c,\n",
        "                                    kernel_size = (1, 1),\n",
        "                                    strides = (s, s),\n",
        "                                    activation = None,\n",
        "                                    use_bias = False,\n",
        "                                    name = name1+'_branch1')\n",
        "        branch1_feats = self.nn.batch_norm(branch1_feats, name2+'_branch1')\n",
        "\n",
        "        branch2a_feats = self.nn.conv2d(inputs,\n",
        "                                     filters = c,\n",
        "                                     kernel_size = (1, 1),\n",
        "                                     strides = (s, s),\n",
        "                                     activation = None,\n",
        "                                     use_bias = False,\n",
        "                                     name = name1+'_branch2a')\n",
        "        branch2a_feats = self.nn.batch_norm(branch2a_feats, name2+'_branch2a')\n",
        "        branch2a_feats = tf.nn.relu(branch2a_feats)\n",
        "\n",
        "        branch2b_feats = self.nn.conv2d(branch2a_feats,\n",
        "                                     filters = c,\n",
        "                                     kernel_size = (3, 3),\n",
        "                                     strides = (1, 1),\n",
        "                                     activation = None,\n",
        "                                     use_bias = False,\n",
        "                                     name = name1+'_branch2b')\n",
        "        branch2b_feats = self.nn.batch_norm(branch2b_feats, name2+'_branch2b')\n",
        "        branch2b_feats = tf.nn.relu(branch2b_feats)\n",
        "\n",
        "        branch2c_feats = self.nn.conv2d(branch2b_feats,\n",
        "                                     filters = 4*c,\n",
        "                                     kernel_size = (1, 1),\n",
        "                                     strides = (1, 1),\n",
        "                                     activation = None,\n",
        "                                     use_bias = False,\n",
        "                                     name = name1+'_branch2c')\n",
        "        branch2c_feats = self.nn.batch_norm(branch2c_feats, name2+'_branch2c')\n",
        "\n",
        "        outputs = branch1_feats + branch2c_feats\n",
        "        outputs = tf.nn.relu(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def resnet_block2(self, inputs, name1, name2, c):\n",
        "        \"\"\" Another basic block of ResNet. \"\"\"\n",
        "        branch2a_feats = self.nn.conv2d(inputs,\n",
        "                                     filters = c,\n",
        "                                     kernel_size = (1, 1),\n",
        "                                     strides = (1, 1),\n",
        "                                     activation = None,\n",
        "                                     use_bias = False,\n",
        "                                     name = name1+'_branch2a')\n",
        "        branch2a_feats = self.nn.batch_norm(branch2a_feats, name2+'_branch2a')\n",
        "        branch2a_feats = tf.nn.relu(branch2a_feats)\n",
        "\n",
        "        branch2b_feats = self.nn.conv2d(branch2a_feats,\n",
        "                                     filters = c,\n",
        "                                     kernel_size = (3, 3),\n",
        "                                     strides = (1, 1),\n",
        "                                     activation = None,\n",
        "                                     use_bias = False,\n",
        "                                     name = name1+'_branch2b')\n",
        "        branch2b_feats = self.nn.batch_norm(branch2b_feats, name2+'_branch2b')\n",
        "        branch2b_feats = tf.nn.relu(branch2b_feats)\n",
        "\n",
        "        branch2c_feats = self.nn.conv2d(branch2b_feats,\n",
        "                                     filters = 4*c,\n",
        "                                     kernel_size = (1, 1),\n",
        "                                     strides = (1, 1),\n",
        "                                     activation = None,\n",
        "                                     use_bias = False,\n",
        "                                     name = name1+'_branch2c')\n",
        "        branch2c_feats = self.nn.batch_norm(branch2c_feats, name2+'_branch2c')\n",
        "\n",
        "        outputs = inputs + branch2c_feats\n",
        "        outputs = tf.nn.relu(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def build_rnn(self):\n",
        "        \"\"\" Build the RNN. \"\"\"\n",
        "        print(\"Building the RNN...\")\n",
        "        config = self.config\n",
        "\n",
        "        # Setup the placeholders\n",
        "        if self.is_train:\n",
        "            contexts = self.conv_feats\n",
        "            sentences = tf.placeholder(\n",
        "                dtype = tf.int32,\n",
        "                shape = [config.batch_size, config.max_caption_length])\n",
        "            masks = tf.placeholder(\n",
        "                dtype = tf.float32,\n",
        "                shape = [config.batch_size, config.max_caption_length])\n",
        "        else:\n",
        "            contexts = tf.placeholder(\n",
        "                dtype = tf.float32,\n",
        "                shape = [config.batch_size, self.num_ctx, self.dim_ctx])\n",
        "            last_memory = tf.placeholder(\n",
        "                dtype = tf.float32,\n",
        "                shape = [config.batch_size, config.num_lstm_units])\n",
        "            last_output = tf.placeholder(\n",
        "                dtype = tf.float32,\n",
        "                shape = [config.batch_size, config.num_lstm_units])\n",
        "            last_word = tf.placeholder(\n",
        "                dtype = tf.int32,\n",
        "                shape = [config.batch_size])\n",
        "\n",
        "        # Setup the word embedding\n",
        "        with tf.variable_scope(\"word_embedding\"):\n",
        "            embedding_matrix = tf.get_variable(\n",
        "                name = 'weights',\n",
        "                shape = [config.vocabulary_size, config.dim_embedding],\n",
        "                initializer = self.nn.fc_kernel_initializer,\n",
        "                regularizer = self.nn.fc_kernel_regularizer,\n",
        "                trainable = self.is_train)\n",
        "\n",
        "        # Setup the LSTM\n",
        "        lstm = tf.nn.rnn_cell.LSTMCell(\n",
        "            config.num_lstm_units,\n",
        "            initializer = self.nn.fc_kernel_initializer)\n",
        "        if self.is_train:\n",
        "            lstm = tf.nn.rnn_cell.DropoutWrapper(\n",
        "                lstm,\n",
        "                input_keep_prob = 1.0-config.lstm_drop_rate,\n",
        "                output_keep_prob = 1.0-config.lstm_drop_rate,\n",
        "                state_keep_prob = 1.0-config.lstm_drop_rate)\n",
        "\n",
        "        # Initialize the LSTM using the mean context\n",
        "        with tf.variable_scope(\"initialize\"):\n",
        "            context_mean = tf.reduce_mean(self.conv_feats, axis = 1)\n",
        "            initial_memory, initial_output = self.initialize(context_mean)\n",
        "            initial_state = initial_memory, initial_output\n",
        "\n",
        "        # Prepare to run\n",
        "        predictions = []\n",
        "        if self.is_train:\n",
        "            alphas = []\n",
        "            cross_entropies = []\n",
        "            predictions_correct = []\n",
        "            num_steps = config.max_caption_length\n",
        "            last_output = initial_output\n",
        "            last_memory = initial_memory\n",
        "            last_word = tf.zeros([config.batch_size], tf.int32)\n",
        "        else:\n",
        "            num_steps = 1\n",
        "        last_state = last_memory, last_output\n",
        "\n",
        "        # Generate the words one by one\n",
        "        for idx in range(num_steps):\n",
        "            # Attention mechanism\n",
        "            with tf.variable_scope(\"attend\"):\n",
        "                alpha = self.attend(contexts, last_output)\n",
        "                context = tf.reduce_sum(contexts*tf.expand_dims(alpha, 2),\n",
        "                                        axis = 1)\n",
        "                if self.is_train:\n",
        "                    tiled_masks = tf.tile(tf.expand_dims(masks[:, idx], 1),\n",
        "                                         [1, self.num_ctx])\n",
        "                    masked_alpha = alpha * tiled_masks\n",
        "                    alphas.append(tf.reshape(masked_alpha, [-1]))\n",
        "\n",
        "            # Embed the last word\n",
        "            with tf.variable_scope(\"word_embedding\"):\n",
        "                word_embed = tf.nn.embedding_lookup(embedding_matrix,\n",
        "                                                    last_word)\n",
        "           # Apply the LSTM\n",
        "            with tf.variable_scope(\"lstm\"):\n",
        "                current_input = tf.concat([context, word_embed], 1)\n",
        "                output, state = lstm(current_input, last_state)\n",
        "                memory, _ = state\n",
        "\n",
        "            # Decode the expanded output of LSTM into a word\n",
        "            with tf.variable_scope(\"decode\"):\n",
        "                expanded_output = tf.concat([output,\n",
        "                                             context,\n",
        "                                             word_embed],\n",
        "                                             axis = 1)\n",
        "                logits = self.decode(expanded_output)\n",
        "                probs = tf.nn.softmax(logits)\n",
        "                prediction = tf.argmax(logits, 1)\n",
        "                predictions.append(prediction)\n",
        "\n",
        "            # Compute the loss for this step, if necessary\n",
        "            if self.is_train:\n",
        "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    labels = sentences[:, idx],\n",
        "                    logits = logits)\n",
        "                masked_cross_entropy = cross_entropy * masks[:, idx]\n",
        "                cross_entropies.append(masked_cross_entropy)\n",
        "\n",
        "                ground_truth = tf.cast(sentences[:, idx], tf.int64)\n",
        "                prediction_correct = tf.where(\n",
        "                    tf.equal(prediction, ground_truth),\n",
        "                    tf.cast(masks[:, idx], tf.float32),\n",
        "                    tf.cast(tf.zeros_like(prediction), tf.float32))\n",
        "                predictions_correct.append(prediction_correct)\n",
        "\n",
        "                last_output = output\n",
        "                last_memory = memory\n",
        "                last_state = state\n",
        "                last_word = sentences[:, idx]\n",
        "\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "        # Compute the final loss, if necessary\n",
        "        if self.is_train:\n",
        "            cross_entropies = tf.stack(cross_entropies, axis = 1)\n",
        "            cross_entropy_loss = tf.reduce_sum(cross_entropies) \\\n",
        "                                 / tf.reduce_sum(masks)\n",
        "\n",
        "            alphas = tf.stack(alphas, axis = 1)\n",
        "            alphas = tf.reshape(alphas, [config.batch_size, self.num_ctx, -1])\n",
        "            attentions = tf.reduce_sum(alphas, axis = 2)\n",
        "            diffs = tf.ones_like(attentions) - attentions\n",
        "            attention_loss = config.attention_loss_factor \\\n",
        "                             * tf.nn.l2_loss(diffs) \\\n",
        "                             / (config.batch_size * self.num_ctx)\n",
        "\n",
        "            reg_loss = tf.losses.get_regularization_loss()\n",
        "\n",
        "            total_loss = cross_entropy_loss + attention_loss + reg_loss\n",
        "\n",
        "            predictions_correct = tf.stack(predictions_correct, axis = 1)\n",
        "            accuracy = tf.reduce_sum(predictions_correct) \\\n",
        "                       / tf.reduce_sum(masks)\n",
        "\n",
        "        self.contexts = contexts\n",
        "        if self.is_train:\n",
        "            self.sentences = sentences\n",
        "            self.masks = masks\n",
        "            self.total_loss = total_loss\n",
        "            self.cross_entropy_loss = cross_entropy_loss\n",
        "            self.attention_loss = attention_loss\n",
        "            self.reg_loss = reg_loss\n",
        "            self.accuracy = accuracy\n",
        "            self.attentions = attentions\n",
        "        else:\n",
        "            self.initial_memory = initial_memory\n",
        "            self.initial_output = initial_output\n",
        "            self.last_memory = last_memory\n",
        "            self.last_output = last_output\n",
        "            self.last_word = last_word\n",
        "            self.memory = memory\n",
        "            self.output = output\n",
        "            self.probs = probs\n",
        "\n",
        "        print(\"RNN built.\")\n",
        "\n",
        "    def initialize(self, context_mean):\n",
        "        \"\"\" Initialize the LSTM using the mean context. \"\"\"\n",
        "        config = self.config\n",
        "        context_mean = self.nn.dropout(context_mean)\n",
        "        if config.num_initalize_layers == 1:\n",
        "            # use 1 fc layer to initialize\n",
        "            memory = self.nn.dense(context_mean,\n",
        "                                   units = config.num_lstm_units,\n",
        "                                   activation = None,\n",
        "                                   name = 'fc_a')\n",
        "            output = self.nn.dense(context_mean,\n",
        "                                   units = config.num_lstm_units,\n",
        "                                   activation = None,\n",
        "                                   name = 'fc_b')\n",
        "        else:\n",
        "            # use 2 fc layers to initialize\n",
        "            temp1 = self.nn.dense(context_mean,\n",
        "                                  units = config.dim_initalize_layer,\n",
        "                                  activation = tf.tanh,\n",
        "                                  name = 'fc_a1')\n",
        "            temp1 = self.nn.dropout(temp1)\n",
        "            memory = self.nn.dense(temp1,\n",
        "                                   units = config.num_lstm_units,\n",
        "                                   activation = None,\n",
        "                                   name = 'fc_a2')\n",
        "\n",
        "            temp2 = self.nn.dense(context_mean,\n",
        "                                  units = config.dim_initalize_layer,\n",
        "                                  activation = tf.tanh,\n",
        "                                  name = 'fc_b1')\n",
        "            temp2 = self.nn.dropout(temp2)\n",
        "            output = self.nn.dense(temp2,\n",
        "                                   units = config.num_lstm_units,\n",
        "                                   activation = None,\n",
        "                                   name = 'fc_b2')\n",
        "        return memory, output\n",
        "\n",
        "    def attend(self, contexts, output):\n",
        "        \"\"\" Attention Mechanism. \"\"\"\n",
        "        config = self.config\n",
        "        reshaped_contexts = tf.reshape(contexts, [-1, self.dim_ctx])\n",
        "        reshaped_contexts = self.nn.dropout(reshaped_contexts)\n",
        "        output = self.nn.dropout(output)\n",
        "        if config.num_attend_layers == 1:\n",
        "            # use 1 fc layer to attend\n",
        "            logits1 = self.nn.dense(reshaped_contexts,\n",
        "                                    units = 1,\n",
        "                                    activation = None,\n",
        "                                    use_bias = False,\n",
        "                                    name = 'fc_a')\n",
        "            logits1 = tf.reshape(logits1, [-1, self.num_ctx])\n",
        "            logits2 = self.nn.dense(output,\n",
        "                                    units = self.num_ctx,\n",
        "                                    activation = None,\n",
        "                                    use_bias = False,\n",
        "                                    name = 'fc_b')\n",
        "            logits = logits1 + logits2\n",
        "        else:\n",
        "            # use 2 fc layers to attend\n",
        "            temp1 = self.nn.dense(reshaped_contexts,\n",
        "                                  units = config.dim_attend_layer,\n",
        "                                  activation = tf.tanh,\n",
        "                                  name = 'fc_1a')\n",
        "            temp2 = self.nn.dense(output,\n",
        "                                  units = config.dim_attend_layer,\n",
        "                                  activation = tf.tanh,\n",
        "                                  name = 'fc_1b')\n",
        "            temp2 = tf.tile(tf.expand_dims(temp2, 1), [1, self.num_ctx, 1])\n",
        "            temp2 = tf.reshape(temp2, [-1, config.dim_attend_layer])\n",
        "            temp = temp1 + temp2\n",
        "            temp = self.nn.dropout(temp)\n",
        "            logits = self.nn.dense(temp,\n",
        "                                   units = 1,\n",
        "                                   activation = None,\n",
        "                                   use_bias = False,\n",
        "                                   name = 'fc_2')\n",
        "            logits = tf.reshape(logits, [-1, self.num_ctx])\n",
        "        alpha = tf.nn.softmax(logits)\n",
        "        return alpha\n",
        "\n",
        "    def decode(self, expanded_output):\n",
        "        \"\"\" Decode the expanded output of the LSTM into a word. \"\"\"\n",
        "        config = self.config\n",
        "        expanded_output = self.nn.dropout(expanded_output)\n",
        "        if config.num_decode_layers == 1:\n",
        "            # use 1 fc layer to decode\n",
        "            logits = self.nn.dense(expanded_output,\n",
        "                                   units = config.vocabulary_size,\n",
        "                                   activation = None,\n",
        "                                   name = 'fc')\n",
        "        else:\n",
        "            # use 2 fc layers to decode\n",
        "            temp = self.nn.dense(expanded_output,\n",
        "                                 units = config.dim_decode_layer,\n",
        "                                 activation = tf.tanh,\n",
        "                                 name = 'fc_1')\n",
        "            temp = self.nn.dropout(temp)\n",
        "            logits = self.nn.dense(temp,\n",
        "                                   units = config.vocabulary_size,\n",
        "                                   activation = None,\n",
        "                                   name = 'fc_2')\n",
        "        return logits\n",
        "\n",
        "    def build_optimizer(self):\n",
        "        \"\"\" Setup the optimizer and training operation. \"\"\"\n",
        "        config = self.config\n",
        "\n",
        "        learning_rate = tf.constant(config.initial_learning_rate)\n",
        "        if config.learning_rate_decay_factor < 1.0:\n",
        "            def _learning_rate_decay_fn(learning_rate, global_step):\n",
        "                return tf.train.exponential_decay(\n",
        "                    learning_rate,\n",
        "                    global_step,\n",
        "                    decay_steps = config.num_steps_per_decay,\n",
        "                    decay_rate = config.learning_rate_decay_factor,\n",
        "                    staircase = True)\n",
        "            learning_rate_decay_fn = _learning_rate_decay_fn\n",
        "        else:\n",
        "            learning_rate_decay_fn = None\n",
        "\n",
        "        with tf.variable_scope('optimizer', reuse = tf.AUTO_REUSE):\n",
        "            if config.optimizer == 'Adam':\n",
        "                optimizer = tf.train.AdamOptimizer(\n",
        "                    learning_rate = config.initial_learning_rate,\n",
        "                    beta1 = config.beta1,\n",
        "                    beta2 = config.beta2,\n",
        "                    epsilon = config.epsilon\n",
        "                    )\n",
        "            elif config.optimizer == 'RMSProp':\n",
        "                optimizer = tf.train.RMSPropOptimizer(\n",
        "                    learning_rate = config.initial_learning_rate,\n",
        "                    decay = config.decay,\n",
        "                    momentum = config.momentum,\n",
        "                    centered = config.centered,\n",
        "                    epsilon = config.epsilon\n",
        "                )\n",
        "            elif config.optimizer == 'Momentum':\n",
        "                optimizer = tf.train.MomentumOptimizer(\n",
        "                    learning_rate = config.initial_learning_rate,\n",
        "                    momentum = config.momentum,\n",
        "                    use_nesterov = config.use_nesterov\n",
        "                )\n",
        "            else:\n",
        "                optimizer = tf.train.GradientDescentOptimizer(\n",
        "                    learning_rate = config.initial_learning_rate\n",
        "                )\n",
        "\n",
        "            opt_op = tf.contrib.layers.optimize_loss(\n",
        "                loss = self.total_loss,\n",
        "                global_step = self.global_step,\n",
        "                learning_rate = learning_rate,\n",
        "                optimizer = optimizer,\n",
        "                clip_gradients = config.clip_gradients,\n",
        "                learning_rate_decay_fn = learning_rate_decay_fn)\n",
        "\n",
        "        self.opt_op = opt_op\n",
        "\n",
        "    def build_summary(self):\n",
        "        \"\"\" Build the summary (for TensorBoard visualization). \"\"\"\n",
        "        with tf.name_scope(\"variables\"):\n",
        "            for var in tf.trainable_variables():\n",
        "                with tf.name_scope(var.name[:var.name.find(\":\")]):\n",
        "                    self.variable_summary(var)\n",
        "\n",
        "        with tf.name_scope(\"metrics\"):\n",
        "            tf.summary.scalar(\"cross_entropy_loss\", self.cross_entropy_loss)\n",
        "            tf.summary.scalar(\"attention_loss\", self.attention_loss)\n",
        "            tf.summary.scalar(\"reg_loss\", self.reg_loss)\n",
        "            tf.summary.scalar(\"total_loss\", self.total_loss)\n",
        "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
        "\n",
        "        with tf.name_scope(\"attentions\"):\n",
        "            self.variable_summary(self.attentions)\n",
        "\n",
        "        self.summary = tf.summary.merge_all()\n",
        "\n",
        "    def variable_summary(self, var):\n",
        "        \"\"\" Build the summary for a variable. \"\"\"\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMEM6HNEjnA"
      },
      "source": [
        "# eval.sh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4WTxwUVqVWQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6e9999b-5a8d-429b-c069-5e718ce9aaca"
      },
      "source": [
        "%cd\n",
        "%cd ../content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ub4Szo6ElbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "557bbdff-784c-41da-b952-1a4606fbb174"
      },
      "source": [
        "%%writefile eval.sh\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "for file in ./models/*.npy\n",
        "do\n",
        "    filename=$(basename \"$file\")\n",
        "    filename=\"${filename%.*}\"\n",
        "    python main.py --phase=eval --model_file=\"${file}\" > \"${filename}.txt\"\n",
        "done\n",
        "exit 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing eval.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9stMQsxhe04K"
      },
      "source": [
        "# Train / Test / Validate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpK7O3Paam6E"
      },
      "source": [
        "#using tensorboard\n",
        "%cd\n",
        "%cd ../content\n",
        "\n",
        "%load_ext tensorboard.notebook\n",
        "# %reload_ext tensorboard.notebook\n",
        "%tensorboard --logdir summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lZRDIPFe3PA"
      },
      "source": [
        "#build model from scratch\n",
        "%cd\n",
        "%cd ../content\n",
        "!python main.py --phase=train \\\n",
        "    --load_cnn \\\n",
        "    --cnn_model_file='gdrive/My Drive/Final Project/TensorFlowImplementation/PreTrained Models/vgg16_no_fc.npy'\\\n",
        "    [--train_cnn]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlQQRqTPiH1p"
      },
      "source": [
        "#loading from a checkpoint\n",
        "%cd\n",
        "%cd ../content\n",
        "!python main.py --phase=train \\\n",
        "    --load \\\n",
        "    --model_file='gdrive/My Drive/Final Project/TensorFlowImplementation/models/111999.npy'\\\n",
        "    [--train_cnn]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUizzAXvqnmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1027f518-0136-4b19-8ebf-ab61f7afcce4"
      },
      "source": [
        "#evaluate a model using val dataset\n",
        "%cd\n",
        "%cd ../content\n",
        "!python main.py --phase=eval \\\n",
        "    --model_file='gdrive/My Drive/Final Project/TensorFlowImplementation/models/111999.npy' \\\n",
        "    --beam_size=3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0711 21:40:17.030117 140549721270144 deprecation_wrapper.py:119] From main.py:71: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0711 21:40:17.030786 140549721270144 deprecation_wrapper.py:119] From main.py:41: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-07-11 21:40:17.064351: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-11 21:40:17.130355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:17.130796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-11 21:40:17.159588: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-11 21:40:17.318303: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-11 21:40:17.411839: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-11 21:40:17.436079: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-11 21:40:17.657246: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-11 21:40:17.803196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-11 21:40:18.195629: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-11 21:40:18.195943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.196534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.196889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-11 21:40:18.221661: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-11 21:40:18.224266: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e56f40 executing computations on platform Host. Devices:\n",
            "2019-07-11 21:40:18.224307: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-11 21:40:18.441880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.442476: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e56840 executing computations on platform CUDA. Devices:\n",
            "2019-07-11 21:40:18.442508: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-07-11 21:40:18.442832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.443225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-11 21:40:18.443313: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-11 21:40:18.443342: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-11 21:40:18.443359: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-11 21:40:18.443398: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-11 21:40:18.443431: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-11 21:40:18.443451: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-11 21:40:18.443484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-11 21:40:18.443588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.444040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.444408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-11 21:40:18.446319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-11 21:40:18.448347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-11 21:40:18.448398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-11 21:40:18.448419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-11 21:40:18.454921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.455415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-11 21:40:18.455774: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-11 21:40:18.455840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "loading annotations into memory...\n",
            "Done (t=0.32s)\n",
            "creating index...\n",
            "index created!\n",
            "Building the vocabulary...\n",
            "loading annotations into memory...\n",
            "Done (t=0.74s)\n",
            "creating index...\n",
            "index created!\n",
            "Filtering the captions by length...\n",
            "100% 414113/414113 [00:48<00:00, 8563.08it/s]\n",
            "creating index...\n",
            "index created!\n",
            "100% 409884/409884 [00:49<00:00, 8352.00it/s]\n",
            "Vocabulary built.\n",
            "Number of words = 5000\n",
            "Building the dataset...\n",
            "Dataset built.\n",
            "Building the CNN...\n",
            "W0711 21:42:01.095440 140549721270144 deprecation_wrapper.py:119] From /content/model.py:29: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0711 21:42:01.096367 140549721270144 deprecation.py:323] From /content/utils/nn.py:70: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0711 21:42:01.342623 140549721270144 deprecation.py:323] From /content/utils/nn.py:83: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "CNN built.\n",
            "Building the RNN...\n",
            "W0711 21:42:01.639646 140549721270144 deprecation_wrapper.py:119] From /content/model.py:220: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0711 21:42:01.639941 140549721270144 deprecation_wrapper.py:119] From /content/model.py:221: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0711 21:42:01.648162 140549721270144 deprecation.py:323] From /content/model.py:231: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0711 21:42:01.650454 140549721270144 deprecation.py:323] From /content/utils/nn.py:114: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0711 21:42:01.703045 140549721270144 deprecation.py:323] From /content/utils/nn.py:105: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0711 21:42:02.095877 140549721270144 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0711 21:42:02.714377 140549721270144 deprecation_wrapper.py:119] From /content/model.py:313: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "RNN built.\n",
            "Loading the model from gdrive/My Drive/Final Project/TensorFlowImplementation/models/111999.npy...\n",
            "  0% 0/47 [00:00<?, ?it/s]2019-07-11 21:42:06.408406: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "100% 47/47 [00:03<00:00, 14.28it/s]\n",
            "47 tensors loaded.\n",
            "Evaluating the model ...\n",
            "config.eval_result_dir: ./val/results/\n",
            "batch:   0% 0/1266 [00:00<?, ?it/s]2019-07-11 21:42:10.229406: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-11 21:42:10.857673: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "batch:  10% 129/1266 [15:17<2:14:13,  7.08s/it]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SexaQQuSqyyv"
      },
      "source": [
        "#test a model using images in the test folder\n",
        "%cd\n",
        "%cd ../content\n",
        "!python main.py --phase=test \\\n",
        "    --model_file='gdrive/My Drive/Final Project/TensorFlowImplementation/models/111999.npy' \\\n",
        "    --beam_size=3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}